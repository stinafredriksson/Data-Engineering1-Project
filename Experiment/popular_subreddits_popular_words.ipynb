{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5b34098-6747-431a-a316-41908fb471e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(145 + 2) / 147]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- content_len: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- normalizedBody: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- summary_len: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "from pyspark.sql.functions import split, explode, lower, col, rank, desc, regexp_replace\n",
    "from pyspark.sql import Window\n",
    "\n",
    "#################### Setup spark session ####################\n",
    "\n",
    "# Stop running SparkSession if it exists\n",
    "if 'spark_session' in locals():\n",
    "    spark_session.stop()\n",
    "    \n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.224:7077\") \\\n",
    "        .appName(\"Experiment\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.executor.cores\",12)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "# Load reddit data\n",
    "reddit_df = spark_session.read.json(\"hdfs://192.168.2.224:9000/user/ubuntu/reddit/corpus-webis-tldr-17.json\")\n",
    "reddit_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7793b3a-0709-4d97-a0ca-77f7df162f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular subreddits, from experiment.ipynb\n",
    "subreddit_list = ['AskReddit', 'relationships', 'leagueoflegends', 'tifu', \n",
    "                  'relationship_advice', 'trees', 'gaming', 'atheism', \n",
    "                  'AdviceAnimals', 'funny']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8fbc5c69-86d4-4601-b927-24b7cb5ae60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+------+----+\n",
      "|subreddit    |word   |count |rank|\n",
      "+-------------+-------+------+----+\n",
      "|AdviceAnimals|being  |14502 |1   |\n",
      "|AdviceAnimals|think  |13109 |2   |\n",
      "|AdviceAnimals|got    |12401 |3   |\n",
      "|AdviceAnimals|really |12397 |4   |\n",
      "|AdviceAnimals|into   |11693 |5   |\n",
      "|AskReddit    |got    |337683|1   |\n",
      "|AskReddit    |back   |299326|2   |\n",
      "|AskReddit    |into   |292977|3   |\n",
      "|AskReddit    |after  |264159|4   |\n",
      "|AskReddit    |really |253384|5   |\n",
      "|atheism      |god    |29140 |1   |\n",
      "|atheism      |think  |25333 |2   |\n",
      "|atheism      |believe|23333 |3   |\n",
      "|atheism      |being  |22745 |4   |\n",
      "|atheism      |any    |22083 |5   |\n",
      "|funny        |into   |11801 |1   |\n",
      "|funny        |being  |11218 |2   |\n",
      "|funny        |think  |10913 |3   |\n",
      "|funny        |got    |10241 |4   |\n",
      "|funny        |-      |9866  |5   |\n",
      "+-------------+-------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get data for only these subreddits\n",
    "filtered_df = reddit_df.filter(col('subreddit').isin(subreddit_list)) # only look at the most popular reddits gotten for \n",
    "\n",
    "# Loading the list of common words, taken from github: https://gist.github.com/deekayen/4148741\n",
    "filtered_words = spark_session.read.text(\"hdfs://192.168.2.224:9000/user/ubuntu/most_common_words.txt\").rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "# The list of filtered words for workers\n",
    "filtered_words_broadcast = spark_session.sparkContext.broadcast(filtered_words)\n",
    "\n",
    "# Extract the words from the specific subreddits, filter out blank space and words with ', make lower case, \n",
    "apostrophe_pattern = r\"\\b\\w*'\\w*\\b\"\n",
    "tokenized_df = filtered_df.withColumn(\"words\", split(lower(filtered_df.body), \" \")) \\\n",
    "                          .withColumn(\"words\", explode(\"words\")) \\\n",
    "                          .withColumn(\"words\", regexp_replace(\"words\", apostrophe_pattern, \"\"))\n",
    "\n",
    "exploded_df = tokenized_df.select(\"subreddit\", explode(split(\"words\", \" \")).alias(\"word\"))\n",
    "exploded_df = exploded_df.filter(exploded_df[\"word\"] != \"\")\n",
    "\n",
    "# Filter out all the words which matches the list of common words\n",
    "filtered_df = exploded_df.filter(~col(\"word\").isin(filtered_words_broadcast.value))\n",
    "\n",
    "# Get word count\n",
    "word_count_df = filtered_df.groupBy(\"subreddit\", \"word\").count()\n",
    "\n",
    "# Rank the words\n",
    "window_spec = Window.partitionBy(\"subreddit\").orderBy(desc(\"count\"))\n",
    "ranked_df = word_count_df.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Top 5 most popular words for each subreddit\n",
    "top_five_words_df = ranked_df.filter(col(\"rank\") <= 5)\n",
    "\n",
    "# Results\n",
    "top_five_words_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d168a2fe-5049-497f-a51b-99e9a60e5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save data\n",
    "top_five_words_df.toPandas().to_csv('subreddit_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c715b688-108d-4b36-b081-2d83f760258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f41590-4e39-47a8-8c10-b055640ee610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
